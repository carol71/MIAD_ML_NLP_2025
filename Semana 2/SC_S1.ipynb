{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c04998-e125-4f67-8495-e819dfcf0d06",
   "metadata": {},
   "source": [
    "# Visualización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cadfa7-ceb9-4b21-a0c0-787966926249",
   "metadata": {},
   "source": [
    "Los árboles de decisión son muy fáciles de visualizar e interpretar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16c09b-4240-4845-8c7a-b77e2434c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn==1.1### When need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fee484c-0996-4b44-bc35-eb682a668cea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_boston\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeRegressor, plot_tree\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\__init__.py:161\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    111\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[1;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the Boston Housing Dataset\n",
    "boston = load_boston()\n",
    "\n",
    "# Select only one predictor variable\n",
    "X = boston.data[:, 5]\n",
    "y = boston.target\n",
    "\n",
    "# Sort the data by X\n",
    "idx = X.argsort()\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# Fit a decision tree with max depth of 1\n",
    "dt1 = DecisionTreeRegressor(max_depth=1)\n",
    "dt1.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "# Fit a decision tree with max depth of 5\n",
    "dt5 = DecisionTreeRegressor(max_depth=5)\n",
    "dt5.fit(X.reshape(-1, 1), y)\n",
    "\n",
    "# Plot the decision trees\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "plot_tree(dt1, ax=ax[0], filled=True, feature_names=['X'])\n",
    "ax[0].set_title(\"Decision tree with max depth of 1\")\n",
    "plot_tree(dt5, ax=ax[1], filled=True, feature_names=['X'])\n",
    "ax[1].set_title(\"Decision tree with max depth of 5\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a5b1d-6703-4b25-8dd8-60b908339c85",
   "metadata": {},
   "source": [
    "# Explicabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420f0355-b34c-4105-955b-d09edf773568",
   "metadata": {},
   "source": [
    "Igualmente, es fácil explicar por qué una observación es clasificada de cierta manera por un árbol de decisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec22cbe-2228-4213-8f8c-90c529324154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "   \n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create a decision tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Plot the decision tree\n",
    "plt.figure(figsize=(15,8))\n",
    "plot_tree(clf, fontsize=10, feature_names=iris.feature_names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9270157-e782-4324-a60f-58fe4157ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to explain why an observation is classified in a certain way\n",
    "def explanation_prediction(clf, observation):\n",
    "    # Traverse the decision tree to explain the prediction for the observation\n",
    "    node_index = 0\n",
    "    while clf.tree_.children_left[node_index] != -1:\n",
    "        feature_index = clf.tree_.feature[node_index]\n",
    "        threshold = clf.tree_.threshold[node_index]\n",
    "        if observation[0][feature_index] <= threshold:\n",
    "            node_index = clf.tree_.children_left[node_index]\n",
    "            print(f\"Go to node {node_index}: {iris.feature_names[feature_index]} <= {threshold}\")\n",
    "        else:\n",
    "            node_index = clf.tree_.children_right[node_index]\n",
    "            print(f\"Go to node {node_index}: {iris.feature_names[feature_index]} > {threshold}\")\n",
    "\n",
    "    # Print the predicted class for the observation\n",
    "    prediction = clf.predict(observation)[0]\n",
    "    print(f\"The observation {observation[0]} is predicted to be in class {iris.target_names[prediction]}\")\n",
    "\n",
    "\n",
    "# Define an observation to explain the prediction for\n",
    "observation = [[0.1, 3.6, 1.4, 4.5]] # this is a new observation for which we want to explain the prediction\n",
    "explanation_prediction(clf, observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ef476-45fe-4c1f-a6b6-d4235fda1988",
   "metadata": {},
   "source": [
    "# Overfitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a168683-7394-4970-95bd-e50a9ab745b0",
   "metadata": {},
   "source": [
    "Modelos de árboles pueden hacer overfitting de los datos muy fácilmente. Son propensos a hacer overfitting en particular con hiperparámetros como la profundidad del árbol o el mínimo de observaciones en cada nodo terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f211bb-4aba-47ac-8978-c1b09b3f1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the Boston Housing Dataset\n",
    "boston = load_boston()\n",
    "\n",
    "# Select only one predictor variable\n",
    "X = boston.data[:, 5]\n",
    "y = boston.target\n",
    "\n",
    "# Sort the data by X\n",
    "idx = X.argsort()\n",
    "X = X[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# Set max_depth values for decision tree\n",
    "max_depths = [2, 5, 10]\n",
    "\n",
    "# Create scatter plot of real values and decision tree graphs\n",
    "fig, axs = plt.subplots(1, 4, figsize=(24, 5))\n",
    "\n",
    "for i, depth in enumerate(max_depths):\n",
    "    # Fit a decision tree with current max_depth value\n",
    "    dt = DecisionTreeRegressor(max_depth=depth)\n",
    "    dt.fit(X.reshape(-1, 1), y)\n",
    "    y_pred = dt.predict(X.reshape(-1, 1))\n",
    "    # Plot the decision tree graph\n",
    "    axs[i].scatter(X, y, alpha=0.5, label='Real Values')\n",
    "    axs[i].plot(X, y_pred, color='red', label=f'Predicted Values (max depth = {depth})')\n",
    "    axs[i].set_xlabel('Predictor Variable')\n",
    "    axs[i].set_ylabel('Target Variable')\n",
    "    axs[i].set_title(f'Max Depth = {depth}')\n",
    "    axs[i].legend()\n",
    "\n",
    "# Fit a linear regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X.reshape(-1, 1), y)\n",
    "y_pred_reg = reg.predict(X.reshape(-1, 1))\n",
    "\n",
    "# Plot for linear regression\n",
    "axs[-1].scatter(X, y, alpha=0.5, label='Real Values')\n",
    "axs[-1].plot(X, y_pred_reg, color='red', label='Predicted Values (Linear Regression)')\n",
    "axs[-1].set_xlabel('Predictor Variable')\n",
    "axs[-1].set_ylabel('Target Variable')\n",
    "axs[-1].set_title('Linear Regression')\n",
    "axs[-1].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6560bb4a-ea21-4c9b-8488-2b67d0aae7bb",
   "metadata": {},
   "source": [
    "# Cambios en árbol con distintos datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701cac8-233c-4e85-84ee-d98d931356ff",
   "metadata": {},
   "source": [
    "Los árboles de decisión pueden cambiar bastante si se cambia el conjunto de entrenamiento de los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f5992-568d-4af4-b1fd-d2a0f8e97530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Select only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Define colors for each class\n",
    "colors = ['blue', 'red', 'green']\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# Train two decision tree classifiers on slightly different training sets\n",
    "tree1 = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "tree1.fit(X[:75], y[:75])\n",
    "\n",
    "tree2 = DecisionTreeClassifier(max_depth=2, random_state=1)\n",
    "tree2.fit(X[25:], y[25:])\n",
    "\n",
    "# Plot the decision boundaries of the two trees\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "for i, tree in enumerate([tree1, tree2]):\n",
    "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax[i].contourf(xx, yy, Z, alpha=0.2, cmap=cmap)\n",
    "    ax[i].scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label=iris.target_names[0])\n",
    "    ax[i].scatter(X[y == 1, 0], X[y == 1, 1], c='red', label=iris.target_names[1])\n",
    "    ax[i].scatter(X[y == 2, 0], X[y == 2, 1], c='green', label=iris.target_names[2])\n",
    "    ax[i].set_xlabel(iris.feature_names[0])\n",
    "    ax[i].set_ylabel(iris.feature_names[1])\n",
    "    ax[i].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc98657-849c-4cbc-b582-dd8b360b8db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "n_samples = 10000\n",
    "\n",
    "# Generate a classification dataset with 2 features and 500 samples\n",
    "X, y = make_classification(n_samples=n_samples, n_features=2, n_informative=2,\n",
    "                            n_redundant=0, n_classes=2, random_state=0)\n",
    "\n",
    "# Train two decision tree classifiers on slightly different training sets\n",
    "tree1 = DecisionTreeClassifier(max_depth=5, random_state=0)\n",
    "tree1.fit(X[:int(0.75*n_samples)], y[:int(0.75*n_samples)])\n",
    "\n",
    "tree2 = DecisionTreeClassifier(max_depth=5, random_state=1)\n",
    "tree2.fit(X[int(0.25*n_samples):], y[int(0.25*n_samples):])\n",
    "\n",
    "# Plot the training data and the decision boundaries of the two trees\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "for i, tree in enumerate([tree1, tree2]):\n",
    "    xx, yy = np.meshgrid(np.linspace(-4, 4, 100), np.linspace(-6, 6, 100))\n",
    "    Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax[i].contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
    "    ax[i].scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu, s=20, edgecolors='k', alpha=0.05)\n",
    "    ax[i].set_title(f\"Tree {i+1}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88c65d-63fb-43f0-9f03-eae33671a8b7",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26690bf2-dca6-40e1-8097-197b592f7aeb",
   "metadata": {},
   "source": [
    "Los árboles de decisión no cambian mucho si las variables predictivas son más o menos extremas. Es decir, si a un punto que está siendo enviado a la rama derecha de un árbol de acuerdo a la variable X se le aumenta el valor a X, el árbol no cambia en absolutamente nada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47bf0c5-1c77-4a46-8714-4031fde6b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Modify the dataset by multiplying the most extreme observations on the first independent variable by a large number\n",
    "multiplier = 1000\n",
    "X_modified = iris.data.copy()\n",
    "X_modified[np.argmax(X_modified[:, 0]), 0] *= multiplier\n",
    "X_modified[np.argmin(X_modified[:, 0]), 0] *= multiplier\n",
    "\n",
    "# Create a decision tree classifier with the modified dataset\n",
    "clf_modified = DecisionTreeClassifier(random_state=0)\n",
    "clf_modified.fit(X_modified, iris.target)\n",
    "\n",
    "# Create a decision tree classifier with the original dataset\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Plot the decision trees side by side\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\n",
    "plot_tree(clf_modified, fontsize=10, ax=axs[0])\n",
    "axs[0].set_title(\"With modified dataset\")\n",
    "plot_tree(clf, fontsize=10, ax=axs[1])\n",
    "axs[1].set_title(\"With original dataset\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68bdb6-54bf-4c64-922c-497c7d1a8acc",
   "metadata": {},
   "source": [
    "# Extrapolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01561775-3c61-4ae6-96e5-abf3371ea123",
   "metadata": {},
   "source": [
    "Los árboles de decisión son malos para extrapolar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118c91cf-5d32-4007-97b9-2a311cbf5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some sample data with a quadratic relationship\n",
    "np.random.seed(42)\n",
    "x_train = np.linspace(0, 10, 100)\n",
    "y_train = x_train**2 + np.random.normal(0, 20, size=100)\n",
    "\n",
    "# Fit a decision tree to the training data\n",
    "tree = DecisionTreeRegressor(max_depth=2)\n",
    "tree.fit(x_train.reshape(-1, 1), y_train)\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "# Plot the training data and the decision tree prediction\n",
    "x_test = np.linspace(-2, 12, 1000)\n",
    "y_test = x_test**2\n",
    "y_pred = tree.predict(x_test.reshape(-1, 1))\n",
    "ax[0].scatter(x_train, y_train, label='Training Data')\n",
    "ax[0].plot(x_test, y_pred, label='Decision Tree Prediction', color='red')\n",
    "ax[0].plot(x_test, y_test, label='True Function')\n",
    "ax[0].legend()\n",
    "\n",
    "x_test = np.linspace(-5, 30, 1000)\n",
    "y_test = x_test**2\n",
    "y_pred = tree.predict(x_test.reshape(-1, 1))\n",
    "ax[1].scatter(x_train, y_train, label='Training Data')\n",
    "ax[1].plot(x_test, y_pred, label='Decision Tree Prediction', color='red')\n",
    "ax[1].plot(x_test, y_test, label='True Function')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db524d7e-7b50-4bb5-a906-4e960003ddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Select only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Train two decision tree classifiers on slightly different training sets\n",
    "tree1 = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "tree1.fit(X[25:], y[25:])\n",
    "\n",
    "# Plot the decision boundaries of the two trees\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\n",
    "multipliers = [1, 6, 20]\n",
    "\n",
    "for i in range(3):\n",
    "    x_min, x_max = X[:, 0].min() - 0.5*multipliers[i], X[:, 0].max() + 0.5*multipliers[i]\n",
    "    y_min, y_max = X[:, 1].min() - 0.5*multipliers[i], X[:, 1].max() + 0.5*multipliers[i]\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    Z = tree1.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax[i].contourf(xx, yy, Z, alpha=0.4)\n",
    "    ax[i].scatter(X[y == 0, 0], X[y == 0, 1], c='blue', label=iris.target_names[0])\n",
    "    ax[i].scatter(X[y == 1, 0], X[y == 1, 1], c='red', label=iris.target_names[1])\n",
    "    ax[i].scatter(X[y == 2, 0], X[y == 2, 1], c='green', label=iris.target_names[2])\n",
    "    ax[i].set_xlabel(iris.feature_names[0])\n",
    "    ax[i].set_ylabel(iris.feature_names[1])\n",
    "    ax[i].legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f282f6-f765-4fa5-8f16-1944577b1bef",
   "metadata": {},
   "source": [
    "# Variance of estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa9429-a880-416b-a1c4-3b05c7b47751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the Boston housing dataset\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.Series(boston.target)\n",
    "\n",
    "# Create 100 decision trees and bagging regressors\n",
    "tree_preds = []\n",
    "bagging_preds = []\n",
    "\n",
    "for i in range(200):\n",
    "    # Resample the data with replacement to create a bootstrap sample\n",
    "    bootstrap_indices = np.random.choice(np.arange(len(X)), size=len(X), replace=True)\n",
    "    X_boot = X.loc[bootstrap_indices]\n",
    "    y_boot = y.loc[bootstrap_indices]\n",
    "\n",
    "    tree = DecisionTreeRegressor(max_depth=4)\n",
    "    tree.fit(X_boot, y_boot)\n",
    "    tree_preds.append(tree.predict([X.iloc[0]])[0])\n",
    "\n",
    "    bag = BaggingRegressor(base_estimator=DecisionTreeRegressor(max_depth=4), n_estimators=100)\n",
    "    bag.fit(X_boot, y_boot)\n",
    "    bagging_preds.append(bag.predict([X.iloc[0]])[0])\n",
    "\n",
    "# Set the same bin edges for both histograms\n",
    "bin_edges = np.linspace(min(tree_preds + bagging_preds), max(tree_preds + bagging_preds), 21)\n",
    "\n",
    "# Plot histograms of the predictions\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.hist(tree_preds, bins=bin_edges)\n",
    "ax1.set_title('Decision Tree Predictions')\n",
    "ax1.set_xlim(bin_edges[0], bin_edges[-1])\n",
    "\n",
    "ax2.hist(bagging_preds, bins=bin_edges)\n",
    "ax2.set_title('Bagging Regressor Predictions (100 trees)')\n",
    "ax2.set_xlim(bin_edges[0], bin_edges[-1])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a016108-2b90-48d1-87fa-808f813dbf18",
   "metadata": {},
   "source": [
    "# Computational Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f69d95-bcf1-40d6-8b29-3a3bc638a7ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define range of dataset sizes to test\n",
    "sizes = np.linspace(10000, 1010000, num=11, dtype=int)\n",
    "\n",
    "# Initialize empty lists to store results\n",
    "times = []\n",
    "complexity = []\n",
    "\n",
    "# Loop through dataset sizes and measure time to fit a decision tree\n",
    "for n in sizes:\n",
    "    print(f\"Dataset size: {n}.\")\n",
    "    # Generate simulated dataset\n",
    "    X = np.random.rand(n, 10)\n",
    "    y = np.random.rand(n)\n",
    "\n",
    "    # Fit decision tree and measure time\n",
    "    start_time = time.time()\n",
    "    model = DecisionTreeRegressor(max_depth=5)\n",
    "    model.fit(X, y)\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "    complexity.append(n)\n",
    "\n",
    "# Plot results\n",
    "plt.plot(complexity, times, 'bo-')\n",
    "plt.xlabel('Dataset size (n)')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
